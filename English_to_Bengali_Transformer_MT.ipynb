{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kartik-Koul/Transformers-for-Machine-Translation/blob/main/English_to_Bengali_Transformer_MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lE7Cfiykq4X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENCODER_LEN = 100\n",
        "DECODER_LEN = 100\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = BATCH_SIZE*4"
      ],
      "metadata": {
        "id": "_3oxwa-5k3uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TnUAV_bk98b",
        "outputId": "a5d20835-4848-4d19-ec8f-e9ab6ddf44f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Machine Translation/train_english.txt', 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "lines = [line.strip() for line in lines]\n",
        "df_english = pd.DataFrame(lines, columns=['english'])\n",
        "\n",
        "with open('/content/drive/MyDrive/Machine Translation/train_bengali.txt', 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "lines = [line.strip() for line in lines]\n",
        "df_bengali = pd.DataFrame(lines, columns=['bengali'])\n",
        "\n",
        "train_df = pd.concat([df_english, df_bengali], axis = 1)\n",
        "mask = (train_df['english'].str.len()>30) & (train_df['english'].str.len()<300)\n",
        "train_df = train_df.loc[mask]\n",
        "train_df = train_df.sample(64000, random_state=1)\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "ab1eqoL3lJ82",
        "outputId": "9c1072c0-b781-40ac-8638-d6f8f465629a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                            english  \\\n",
              "1149759                                                                                                                                                                                                           and haughtily goes to his people.   \n",
              "5141405                                                                               If an honest self - examination proves that we have slackened the hand somewhat, we do well to recall the invigorating words spoken by the prophet Zephaniah.   \n",
              "6711996                                                                                                       satan is indeed your enemy. therefore take him for an enemy. He calls his party so that they will become the companions of the Blaze.   \n",
              "6983923                                                                                                                                                                       The batsman that he dismissed the most in the series was Steve Smith.   \n",
              "2145897  Say: 'I do not have the power to acquire benefits or to avert harm from myself, except by the Will of Allah. Had I possessed knowledge of the unseen, I would have availed myself of much that is good, and no harm would have touched me.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                         bengali  \n",
              "1149759                                                                                                                                                                                                                                      তারপর সে তার স্বজনগণের কাছে গিয়েছিল গর্ব করতে করতে।  \n",
              "5141405                                                                                                                                      এক অকপট আত্মপরীক্ষা যদি প্রমাণ দেয় যে, আমরা আমাদের হস্ত কিছুটা শিথিল করে ফেলেছি, তাহলে ভাববাদী সফনিয়ের উদ্দীপনামূলক কথাগুলো আমাদের স্মরণ করা উচিত ।  \n",
              "6711996                                                                                                                                                                                    শয়তান তোমাদের শত্রু. অতএব তাকে শত্রু রূপেই গ্রহণ কর। সে তার দলবলকে আহবান করে যেন তারা জাহান্নামী হয়।  \n",
              "6983923                                                                                                                                                                                                                            ভরসা তখন এই সিরিজে সব চেয়ে বিধ্বংসী ব্যাটসম্যান স্টিভ স্মিথ।  \n",
              "2145897  আপনি বলে দিন, আমি আমার নিজের কল্যাণ সাধনের এবং অকল্যাণ সাধনের মালিক নই, কিন্তু যা আল্লাহ চান। আর আমি যদি গায়বের কথা জেনে নিতে পারতাম, তাহলে বহু মঙ্গল অর্জন করে নিতে পারতাম, ফলে আমার কোন অমঙ্গল কখনও হতে পারত না। আমি তো শুধুমাত্র একজন ভীতি প্রদর্শক ও সুসংবাদদাতা ঈমানদারদের জন্য।  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-624d2164-9e0b-4354-898b-dffeea2c4599\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english</th>\n",
              "      <th>bengali</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1149759</th>\n",
              "      <td>and haughtily goes to his people.</td>\n",
              "      <td>তারপর সে তার স্বজনগণের কাছে গিয়েছিল গর্ব করতে করতে।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5141405</th>\n",
              "      <td>If an honest self - examination proves that we have slackened the hand somewhat, we do well to recall the invigorating words spoken by the prophet Zephaniah.</td>\n",
              "      <td>এক অকপট আত্মপরীক্ষা যদি প্রমাণ দেয় যে, আমরা আমাদের হস্ত কিছুটা শিথিল করে ফেলেছি, তাহলে ভাববাদী সফনিয়ের উদ্দীপনামূলক কথাগুলো আমাদের স্মরণ করা উচিত ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6711996</th>\n",
              "      <td>satan is indeed your enemy. therefore take him for an enemy. He calls his party so that they will become the companions of the Blaze.</td>\n",
              "      <td>শয়তান তোমাদের শত্রু. অতএব তাকে শত্রু রূপেই গ্রহণ কর। সে তার দলবলকে আহবান করে যেন তারা জাহান্নামী হয়।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6983923</th>\n",
              "      <td>The batsman that he dismissed the most in the series was Steve Smith.</td>\n",
              "      <td>ভরসা তখন এই সিরিজে সব চেয়ে বিধ্বংসী ব্যাটসম্যান স্টিভ স্মিথ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2145897</th>\n",
              "      <td>Say: 'I do not have the power to acquire benefits or to avert harm from myself, except by the Will of Allah. Had I possessed knowledge of the unseen, I would have availed myself of much that is good, and no harm would have touched me.</td>\n",
              "      <td>আপনি বলে দিন, আমি আমার নিজের কল্যাণ সাধনের এবং অকল্যাণ সাধনের মালিক নই, কিন্তু যা আল্লাহ চান। আর আমি যদি গায়বের কথা জেনে নিতে পারতাম, তাহলে বহু মঙ্গল অর্জন করে নিতে পারতাম, ফলে আমার কোন অমঙ্গল কখনও হতে পারত না। আমি তো শুধুমাত্র একজন ভীতি প্রদর্শক ও সুসংবাদদাতা ঈমানদারদের জন্য।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-624d2164-9e0b-4354-898b-dffeea2c4599')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-624d2164-9e0b-4354-898b-dffeea2c4599 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-624d2164-9e0b-4354-898b-dffeea2c4599');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-89cce59d-7a8b-48c0-8e24-4b744f6cd133\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-89cce59d-7a8b-48c0-8e24-4b744f6cd133')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-89cce59d-7a8b-48c0-8e24-4b744f6cd133 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 64000,\n  \"fields\": [\n    {\n      \"column\": \"english\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 62234,\n        \"samples\": [\n          \"No complaint was registered by the family.\",\n          \"The IPS 20 model comes with a 1.3GHz quad core processor along with 1GB RAM and 8GB inbuilt memory.\",\n          \"The innovative role of Grameen Bank has had tremendous positive impact on the development of female entrepreneurs.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bengali\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 63783,\n        \"samples\": [\n          \"\\u0986\\u09ae\\u099f\\u09cd\\u09b0\\u09be\\u0995\\u09c7\\u09b0 \\u09aa\\u09cd\\u09b0\\u09c7\\u09b8\\u09bf\\u09a1\\u09c7\\u09a8\\u09cd\\u099f \\u09aa\\u09b2 \\u09b0\\u09bf\\u09b8\\u099f\\u09cd\\u09b0\\u09be\\u09aa \\u09ac\\u09b2\\u09c7\\u09a8 \\u09af\\u09c7 \\u09aa\\u09cd\\u09b0\\u09a4\\u09bf \\u09ac\\u099b\\u09b0\\u09c7 \\u09ea.\\u09eb \\u09ae\\u09bf\\u09b2\\u09bf\\u09af\\u09bc\\u09a8 \\u09a1\\u09b2\\u09be\\u09b0 \\u0996\\u09b0\\u099a \\u09b9\\u09ac\\u09c7, \\u098f\\u09ac\\u0982 \\u09aa\\u09cd\\u09b0\\u09a5\\u09ae \\u09ac\\u099b\\u09b0\\u09c7 \\u09b2\\u09be\\u09ad \\u09b9\\u09ac\\u09c7 \\u09ef\\u09e6\\u09e6,\\u09e6\\u09e6\\u09e6 \\u09a1\\u09b2\\u09be\\u09b0\\u0964\",\n          \"\\\"\\\"\\\"\\u09a6\\u09c7\\u09b6\\u09c7\\u09b0 \\u0995\\u09b0\\u09be\\u09b0\\u09cb\\u09aa \\u09ac\\u09cd\\u09af\\u09ac\\u09b8\\u09cd\\u09a5\\u09be, \\u0986\\u09b0\\u09cd\\u09a5\\u09bf\\u0995 \\u0986\\u0987\\u09a8 \\u09aa\\u09cd\\u09b0\\u09a3\\u09af\\u09bc\\u09a8, \\u0986\\u09b0\\u09cd\\u09a5\\u09bf\\u0995 \\u09aa\\u09cd\\u09b0\\u09a4\\u09bf\\u09b7\\u09cd\\u09a0\\u09be\\u09a8\\u09b8\\u09ae\\u09c2\\u09b9, \\u09ae\\u09c2\\u09b2\\u09a7\\u09a8 \\u09ac\\u09be\\u099c\\u09be\\u09b0, \\u0995\\u09c7\\u09a8\\u09cd\\u09a6\\u09cd\\u09b0\\u09c0\\u09af\\u09bc \\u0993 \\u09b0\\u09be\\u099c\\u09cd\\u09af \\u09b8\\u09b0\\u0995\\u09be\\u09b0\\u09c7\\u09b0 \\u0985\\u09b0\\u09cd\\u09a5\\u09ac\\u09cd\\u09af\\u09ac\\u09b8\\u09cd\\u09a5\\u09be \\u098f\\u09ac\\u0982 \\u0995\\u09c7\\u09a8\\u09cd\\u09a6\\u09cd\\u09b0\\u09c0\\u09af\\u09bc \\u09ac\\u09be\\u099c\\u09c7\\u099f \\u098f\\u0987 \\u09ae\\u09a8\\u09cd\\u09a4\\u09cd\\u09b0\\u0995\\u0987 \\u09a8\\u09bf\\u09af\\u09bc\\u09a8\\u09cd\\u09a4\\u09cd\\u09b0\\u09a3 \\u0995\\u09b0\\u09c7 \\u09a5\\u09be\\u0995\\u09c7\\u0964\\\"\\\"\\\"\",\n          \"\\u0987\\u09b8\\u09cb\\u09af\\u09bc\\u09be\\u09a4\\u09bf\\u09a8\\u09bf\\u09b0 \\u09aa\\u09cd\\u09b0\\u09a7\\u09be\\u09a8 \\u09ac\\u09bf\\u09a6\\u09c7\\u09b6\\u09c0 \\u09ac\\u09be\\u09a3\\u09bf\\u099c\\u09cd\\u09af\\u09bf\\u0995 \\u0985\\u0982\\u09b6\\u09c0\\u09a6\\u09be\\u09b0 \\u09ae\\u09be\\u09b0\\u09cd\\u0995\\u09bf\\u09a8 \\u09af\\u09c1\\u0995\\u09cd\\u09a4\\u09b0\\u09be\\u09b7\\u09cd\\u099f\\u09cd\\u09b0 \\u098f\\u09ac\\u0982 \\u0987\\u0989\\u09b0\\u09cb\\u09aa\\u09c0\\u09af\\u09bc \\u0987\\u0989\\u09a8\\u09bf\\u09af\\u09bc\\u09a8\\u0964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "VYtiJiMHrJF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng = train_df['english']\n",
        "ben = train_df['bengali']\n",
        "eng = eng.apply(lambda x: \"<SOS> \" + str(x) + \" <EOS>\")\n",
        "ben = ben.apply(lambda x: \"<SOS> \"+ x + \" <EOS>\")"
      ],
      "metadata": {
        "id": "55Ot4HeXlhvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'\n",
        "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
        "ben_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = filters, oov_token=oov_token)\n",
        "eng_tokenizer.fit_on_texts(eng)\n",
        "ben_tokenizer.fit_on_texts(ben)\n",
        "inputs = eng_tokenizer.texts_to_sequences(eng)\n",
        "targets = ben_tokenizer.texts_to_sequences(ben)"
      ],
      "metadata": {
        "id": "EkWOymcHl3MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENCODER_VOCAB = len(eng_tokenizer.word_index) + 1\n",
        "DECODER_VOCAB = len(ben_tokenizer.word_index) + 1\n",
        "print(ENCODER_VOCAB, DECODER_VOCAB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ZWOiWqmBty",
        "outputId": "f265e97d-0a7a-4fc6-e9e4-2b1bd8e91728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49490 96253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
        "inputs = tf.cast(inputs, dtype=tf.int64)\n",
        "targets = tf.cast(targets, dtype=tf.int64)"
      ],
      "metadata": {
        "id": "YOVo1mQ9mG_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "nSVkoB47mOZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "RCsbPsH1mS2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "metadata": {
        "id": "jkbI5Cm-mZGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "metadata": {
        "id": "opk7StPPmdWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "LXKUDDaWmiG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        return x, attention_weights"
      ],
      "metadata": {
        "id": "5GG452IBmmHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "metadata": {
        "id": "Slub2ycGmq5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 6\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "EPOCHS = 5"
      ],
      "metadata": {
        "id": "y240i_tEmuts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "oOrWKFtxnCfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
      ],
      "metadata": {
        "id": "1uVCEVTvnGch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "XFE1_7kPnJHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "x1UIM-ETnznr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=ENCODER_VOCAB,\n",
        "    target_vocab_size=DECODER_VOCAB,\n",
        "    pe_input=1000,\n",
        "    pe_target=1000,\n",
        "    rate=dropout_rate)"
      ],
      "metadata": {
        "id": "QTUg-ziGpHHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "Ok2dqavqpJwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "CBAnzyu5pLpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp,\n",
        "            True,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))\n"
      ],
      "metadata": {
        "id": "9QP7ox8VpNME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "OOKx70yGpQnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5353a2-e1d1-4ea1-8d49-052789fcf2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 9.0164 Accuracy 0.0771\n",
            "Epoch 1 Batch 200 Loss 8.1671 Accuracy 0.0779\n",
            "Epoch 1 Batch 400 Loss 8.1496 Accuracy 0.0785\n",
            "Epoch 1 Batch 600 Loss 8.1337 Accuracy 0.0793\n",
            "Epoch 1 Batch 800 Loss 8.1048 Accuracy 0.0799\n",
            "Epoch 1 Batch 1000 Loss 8.0820 Accuracy 0.0805\n",
            "Epoch 1 Batch 1200 Loss 8.0528 Accuracy 0.0811\n",
            "Epoch 1 Batch 1400 Loss 8.0253 Accuracy 0.0818\n",
            "Epoch 1 Batch 1600 Loss 7.9944 Accuracy 0.0824\n",
            "Epoch 1 Batch 1800 Loss 7.9693 Accuracy 0.0830\n",
            "Epoch 1 Loss 7.9547 Accuracy 0.0835\n",
            "Time taken for 1 epoch: 612.4200406074524 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.8775 Accuracy 0.0835\n",
            "Epoch 2 Batch 200 Loss 7.6774 Accuracy 0.0841\n",
            "Epoch 2 Batch 400 Loss 7.6970 Accuracy 0.0847\n",
            "Epoch 2 Batch 600 Loss 7.6979 Accuracy 0.0853\n",
            "Epoch 2 Batch 800 Loss 7.6712 Accuracy 0.0860\n",
            "Epoch 2 Batch 1000 Loss 7.6513 Accuracy 0.0865\n",
            "Epoch 2 Batch 1200 Loss 7.6293 Accuracy 0.0871\n",
            "Epoch 2 Batch 1400 Loss 7.6130 Accuracy 0.0877\n",
            "Epoch 2 Batch 1600 Loss 7.5905 Accuracy 0.0883\n",
            "Epoch 2 Batch 1800 Loss 7.5703 Accuracy 0.0888\n",
            "Epoch 2 Loss 7.5559 Accuracy 0.0893\n",
            "Time taken for 1 epoch: 614.8376343250275 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 7.4299 Accuracy 0.0893\n",
            "Epoch 3 Batch 200 Loss 7.3371 Accuracy 0.0899\n",
            "Epoch 3 Batch 400 Loss 7.3470 Accuracy 0.0904\n",
            "Epoch 3 Batch 600 Loss 7.3590 Accuracy 0.0909\n",
            "Epoch 3 Batch 800 Loss 7.3456 Accuracy 0.0914\n",
            "Epoch 3 Batch 1000 Loss 7.3354 Accuracy 0.0919\n",
            "Epoch 3 Batch 1200 Loss 7.3245 Accuracy 0.0923\n",
            "Epoch 3 Batch 1400 Loss 7.3177 Accuracy 0.0928\n",
            "Epoch 3 Batch 1600 Loss 7.3060 Accuracy 0.0933\n",
            "Epoch 3 Batch 1800 Loss 7.2942 Accuracy 0.0938\n",
            "Epoch 3 Loss 7.2871 Accuracy 0.0943\n",
            "Time taken for 1 epoch: 615.6160941123962 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 7.2085 Accuracy 0.0943\n",
            "Epoch 4 Batch 200 Loss 7.1530 Accuracy 0.0947\n",
            "Epoch 4 Batch 400 Loss 7.1663 Accuracy 0.0952\n",
            "Epoch 4 Batch 600 Loss 7.1743 Accuracy 0.0956\n",
            "Epoch 4 Batch 800 Loss 7.1695 Accuracy 0.0960\n",
            "Epoch 4 Batch 1000 Loss 7.1642 Accuracy 0.0964\n",
            "Epoch 4 Batch 1200 Loss 7.1571 Accuracy 0.0968\n",
            "Epoch 4 Batch 1400 Loss 7.1552 Accuracy 0.0972\n",
            "Epoch 4 Batch 1600 Loss 7.1476 Accuracy 0.0976\n",
            "Epoch 4 Batch 1800 Loss 7.1425 Accuracy 0.0980\n",
            "Epoch 4 Loss 7.1391 Accuracy 0.0984\n",
            "Time taken for 1 epoch: 622.6637840270996 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 7.1885 Accuracy 0.0984\n",
            "Epoch 5 Batch 200 Loss 7.0562 Accuracy 0.0987\n",
            "Epoch 5 Batch 400 Loss 7.0604 Accuracy 0.0991\n",
            "Epoch 5 Batch 600 Loss 7.0690 Accuracy 0.0995\n",
            "Epoch 5 Batch 800 Loss 7.0652 Accuracy 0.0998\n",
            "Epoch 5 Batch 1000 Loss 7.0618 Accuracy 0.1002\n",
            "Epoch 5 Batch 1200 Loss 7.0564 Accuracy 0.1005\n",
            "Epoch 5 Batch 1400 Loss 7.0546 Accuracy 0.1009\n",
            "Epoch 5 Batch 1600 Loss 7.0475 Accuracy 0.1012\n",
            "Epoch 5 Batch 1800 Loss 7.0434 Accuracy 0.1015\n",
            "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
            "Epoch 5 Loss 7.0406 Accuracy 0.1019\n",
            "Time taken for 1 epoch: 624.4151358604431 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Sir, there were computational problems in working with larger values for\n",
        " the batch size, number of epochs, number of dimensions.\n",
        "  Hence, we have trained on smaller hyperparameters only. '''"
      ],
      "metadata": {
        "id": "nlFNYClTkkFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(text):\n",
        "    text = eng_tokenizer.texts_to_sequences([text])\n",
        "    text = tf.keras.preprocessing.sequence.pad_sequences(text, maxlen=ENCODER_LEN,\n",
        "                                                                   padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(text[0], 0)\n",
        "\n",
        "    decoder_input = [ben_tokenizer.word_index['<sos>']]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "\n",
        "    for i in range(DECODER_LEN):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input,\n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == ben_tokenizer.word_index['<eos>']:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "metadata": {
        "id": "0y9dOcyIpUNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(eng_text):\n",
        "    ben_text = evaluate(text=eng_text)[0].numpy()\n",
        "    ben_text = np.expand_dims(ben_text[1:], 0)\n",
        "    return ben_tokenizer.sequences_to_texts(ben_text)[0]"
      ],
      "metadata": {
        "id": "00F-oqfotM3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is a problem\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Fndd9zMKi022",
        "outputId": "ddb9bc74-bb4c-4eae-a176-8cf194d591c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'কিন্তু এই ধরনের সমস্যা ছিল'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"What is your name\")"
      ],
      "metadata": {
        "id": "-ZZpuprqJ5uh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "72d4812c-a90a-4016-95c8-7f0306523bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'তুমি কি কী'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is our group\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UPf74tJTilJA",
        "outputId": "33ceb0ac-39e2-41d1-e3f1-2f2b5bc79919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'আমি আমাদের জন্য আমাদের জন্য আমাদের জন্য আমাদের কাজ করতে হবে।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}